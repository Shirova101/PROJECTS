# Comparative study
A comprehensive machine learning experiment was conducted to evaluate the performance of various classifiers on different datasets. 
The study encompassed four artificial neural networks, each built with distinct architectures, varying the number of hidden layers 
and activation functions. Python frameworks, specifically TensorFlow and Keras, were utilized to implement and train these models on the MNIST dataset,
which comprises 70,000 images of handwritten digits spanning the range of 0 to 9. The neural network models were rigorously tested and
compared to assess their classification capabilities. To complement this analysis,a Naive Bayes classifier was also developed. This involved
implementing functions to calculate the prior probability for each class and the conditional probabilities of each feature given each class.

For the large 14-feature dataset, extensive data preprocessing techniques were employed to ensure high-quality inputs for training and testing the machine learning models. 
Numpy and Pandas, renowned Python libraries, were instrumental in facilitating data manipulation and analysis during this process.

To accurately gauge the classifiers' effectiveness, performance metrics like accuracy, precision, recall, and F1-score were computed.
The evaluation criteria allowed for a thorough examination of each classifier's strengths and weaknesses, offering valuable insights into their suitability for various real-world applications.

This research project showcases a professional approach to developing and assessing machine learning models, making it an essential contribution to the field of data science and artificial intelligence.






